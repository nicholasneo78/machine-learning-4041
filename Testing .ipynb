{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "surrounded-gibraltar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for garbage collection\n",
    "import gc\n",
    "\n",
    "# for warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# utility libraries\n",
    "import os\n",
    "import copy\n",
    "import tqdm\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import cv2, random, time, shutil, csv\n",
    "import tensorflow as tf \n",
    "import math\n",
    "\n",
    "# keras libraries\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization, Dense, GlobalAveragePooling2D, Lambda, Dropout, InputLayer, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "studied-treat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device:/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# checking if GPU is being used for training\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "if tf.test.gpu_device_name(): \n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"GPU is not detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "conditional-elephant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of classes read - 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10222/10222 [00:27<00:00, 371.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# set image size here\n",
    "img_size = 363\n",
    "data_dir = r'C:\\Users\\Dreamcore\\Documents\\machine-learning-4041-main\\Datasets'\n",
    "data_df = pd.read_csv(os.path.join(data_dir, 'labels.csv'))\n",
    "class_names = sorted(data_df['breed'].unique())\n",
    "print(f\"No. of classes read - {len(class_names)}\")\n",
    "time.sleep(1)\n",
    "\n",
    "images_list = sorted(os.listdir(os.path.join(data_dir, 'train')))\n",
    "X = []\n",
    "Y = []\n",
    "i = 0\n",
    "for image in tqdm.tqdm(images_list[:10222]):\n",
    "    cls_name = data_df[data_df['id'] == image[:-4]].iloc[0,1]\n",
    "    cls_index = int(class_names.index(cls_name)) \n",
    "\n",
    "    # Reading RGB Images\n",
    "    image_path = os.path.join(data_dir, 'train',image)\n",
    "    orig_image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "    res_image = cv2.resize(orig_image,(img_size, img_size))\n",
    "    X.append(res_image)\n",
    "    Y.append(cls_index)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "latter-jefferson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10222 10222\n",
      "(10222, 363, 363, 3) (10222, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting to arrays\n",
    "print(len(X), len(Y))\n",
    "Xarr = np.array(X)\n",
    "Yarr = np.array(Y).reshape(-1,1)\n",
    "\n",
    "del(X)\n",
    "print(Xarr.shape, Yarr.shape)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "unlimited-green",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10222, 363, 363, 3) (10222, 120)\n"
     ]
    }
   ],
   "source": [
    "# converting labels to one hot\n",
    "Yarr_hot = to_categorical(Y)\n",
    "print(Xarr.shape, Yarr_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acting-helicopter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE EXTRACTION OF TRAINING ARRAYS\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "def get_features(model_name, data_preprocessor, data):\n",
    "    '''\n",
    "    1- Create a feature extractor to extract features from the data.\n",
    "    2- Returns the extracted features and the feature extractor.\n",
    "\n",
    "    '''\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "\n",
    "\n",
    "    def preprocess(x):\n",
    "        x = tf.image.random_flip_left_right(x)\n",
    "        x = tf.image.random_brightness(x, 0.5)\n",
    "        return x\n",
    "\n",
    "    ds = dataset.map(preprocess, num_parallel_calls=AUTO).batch(64)\n",
    "\n",
    "    input_size = data.shape[1:]\n",
    "    #Prepare pipeline.\n",
    "    input_layer = Input(input_size)\n",
    "    preprocessor = Lambda(data_preprocessor)(input_layer)\n",
    "\n",
    "    base_model = model_name(weights='imagenet', include_top=False,\n",
    "                                input_shape=input_size)(preprocessor)\n",
    "\n",
    "    avg = GlobalAveragePooling2D()(base_model)\n",
    "    feature_extractor = Model(inputs = input_layer, outputs = avg)\n",
    "\n",
    "\n",
    "    #Extract feature.\n",
    "    feature_maps = feature_extractor.predict(ds, verbose=1)\n",
    "    print('Feature maps shape: ', feature_maps.shape)\n",
    "    \n",
    "    # deleting variables\n",
    "    del(feature_extractor, base_model, preprocessor, dataset)\n",
    "    gc.collect()\n",
    "    return feature_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "false-suspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE EXTRACTION OF VALIDAION AND TESTING ARRAYS\n",
    "def get_valfeatures(model_name, data_preprocessor, data):\n",
    "    '''\n",
    "    Same as above except not image augmentations applied.\n",
    "    Used for feature extraction of validation and testing.\n",
    "    '''\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "\n",
    "    ds = dataset.batch(64)\n",
    "\n",
    "    input_size = data.shape[1:]\n",
    "    #Prepare pipeline.\n",
    "    input_layer = Input(input_size)\n",
    "    preprocessor = Lambda(data_preprocessor)(input_layer)\n",
    "\n",
    "    base_model = model_name(weights='imagenet', include_top=False,\n",
    "                                input_shape=input_size)(preprocessor)\n",
    "\n",
    "    avg = GlobalAveragePooling2D()(base_model)\n",
    "    feature_extractor = Model(inputs = input_layer, outputs = avg)\n",
    "    #Extract feature.\n",
    "    feature_maps = feature_extractor.predict(ds, verbose=1)\n",
    "    print('Feature maps shape: ', feature_maps.shape)\n",
    "    return feature_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "specified-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RETURNING CONCATENATED FEATURES USING MODELS AND PREPROCESSORS\n",
    "def get_concat_features(feat_func, models, preprocs, array):\n",
    "\n",
    "    print(f\"Beggining extraction with {feat_func.__name__}\\n\")\n",
    "    feats_list = []\n",
    "\n",
    "    for i in range(len(models)):\n",
    "        \n",
    "        print(f\"\\nStarting feature extraction with {models[i].__name__} using {preprocs[i].__name__}\\n\")\n",
    "        # applying the above function and storing in list\n",
    "        feats_list.append(feat_func(models[i], preprocs[i], array))\n",
    "\n",
    "    # features concatenating\n",
    "    final_feats = np.concatenate(feats_list, axis=-1)\n",
    "    # memory saving\n",
    "    del(feats_list, array)\n",
    "    gc.collect()\n",
    "\n",
    "    return final_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "turkish-subscriber",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINING models and preprocessors imports \n",
    "\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "inception_preprocessor = preprocess_input\n",
    "\n",
    "from tensorflow.keras.applications.xception import Xception, preprocess_input\n",
    "xception_preprocessor = preprocess_input\n",
    "\n",
    "from tensorflow.keras.applications.nasnet import NASNetLarge, preprocess_input\n",
    "nasnet_preprocessor = preprocess_input\n",
    "\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
    "inc_resnet_preprocessor = preprocess_input\n",
    "\n",
    "models = [InceptionV3,  InceptionResNetV2, Xception, ]\n",
    "preprocs = [inception_preprocessor,  inc_resnet_preprocessor, \n",
    "            xception_preprocessor, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "unnecessary-millennium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beggining extraction with get_features\n",
      "\n",
      "\n",
      "Starting feature extraction with InceptionV3 using preprocess_input\n",
      "\n",
      "160/160 [==============================] - 29s 145ms/step\n",
      "Feature maps shape:  (10222, 2048)\n",
      "\n",
      "Starting feature extraction with InceptionResNetV2 using preprocess_input\n",
      "\n",
      "160/160 [==============================] - 69s 407ms/step\n",
      "Feature maps shape:  (10222, 1536)\n",
      "\n",
      "Starting feature extraction with Xception using preprocess_input\n",
      "\n",
      "160/160 [==============================] - 43s 252ms/step\n",
      "Feature maps shape:  (10222, 2048)\n",
      "Final feature maps shape (10222, 5632)\n"
     ]
    }
   ],
   "source": [
    "# calculating features of the data\n",
    "\n",
    "final_train_features = get_concat_features(get_features, models, preprocs, Xarr)\n",
    "\n",
    "#del(x_train, )\n",
    "gc.collect()\n",
    "print('Final feature maps shape', final_train_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "corrected-romantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "EarlyStop_callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True,\n",
    "                                                   verbose=0)\n",
    "\n",
    "my_callback=[EarlyStop_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "union-trauma",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting fold 1\n",
      "\n",
      "Training...\n",
      "Evaluating model ...\n",
      "107/107 [==============================] - 0s 915us/step - loss: 0.2159 - accuracy: 0.9337\n",
      "213/213 [==============================] - 0s 868us/step - loss: 0.0983 - accuracy: 0.9774\n",
      "\n",
      "Starting fold 2\n",
      "\n",
      "Training...\n",
      "Evaluating model ...\n",
      "107/107 [==============================] - 0s 887us/step - loss: 0.2385 - accuracy: 0.9266\n",
      "213/213 [==============================] - 0s 892us/step - loss: 0.1104 - accuracy: 0.9742\n",
      "\n",
      "Starting fold 3\n",
      "\n",
      "Training...\n",
      "Evaluating model ...\n",
      "107/107 [==============================] - 0s 877us/step - loss: 0.2228 - accuracy: 0.9337\n",
      "213/213 [==============================] - 0s 892us/step - loss: 0.0906 - accuracy: 0.9743\n",
      "\n",
      " CV Score -\n",
      "\n",
      "TrainAccuracy - 0.9752984841664633\n",
      "\n",
      "TrainLoss - 0.09978195776542027\n",
      "\n",
      "ValAccuracy - 0.9313243627548218\n",
      "\n",
      "ValLoss - 0.22573373715082803\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "splits = list(StratifiedKFold(n_splits=3, shuffle=True, random_state=10).split(final_train_features, Y))\n",
    "\n",
    "trained_models = []\n",
    "val_accuracy = []\n",
    "val_losses = []\n",
    "train_accuracy = []\n",
    "train_losses = []\n",
    "\n",
    "#Prepare And Train DNN model\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(splits): \n",
    "\n",
    "    print(f\"\\nStarting fold {i+1}\\n\")\n",
    "    x_train_fold = final_train_features[train_idx, :]\n",
    "    y_train_fold = Yarr_hot[train_idx, :]\n",
    "    x_val_fold = final_train_features[valid_idx]\n",
    "    y_val_fold = Yarr_hot[valid_idx, :]\n",
    "\n",
    "    dnn = keras.models.Sequential([\n",
    "        InputLayer(final_train_features.shape[1:]),\n",
    "        Dropout(0.7),\n",
    "        Dense(120, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    dnn.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    print(\"Training...\")\n",
    "    #Train simple DNN on extracted features.\n",
    "    h = dnn.fit(x_train_fold, y_train_fold,\n",
    "                batch_size=128,\n",
    "                epochs=80,\n",
    "                verbose=0,\n",
    "                validation_data = (x_val_fold, y_val_fold),\n",
    "                callbacks=my_callback)  # max 95.07\n",
    "\n",
    "    print(\"Evaluating model ...\")\n",
    "    model_res_val = dnn.evaluate(x_val_fold, y_val_fold)\n",
    "    model_res_train = dnn.evaluate(x_train_fold, y_train_fold)\n",
    "    train_accuracy.append(model_res_train[1])\n",
    "    train_losses.append(model_res_train[0])\n",
    "    val_accuracy.append(model_res_val[1])\n",
    "    val_losses.append(model_res_val[0])\n",
    "    trained_models.append(dnn)\n",
    "\n",
    "print('\\n CV Score -')\n",
    "print(f\"\\nTrainAccuracy - {sum(train_accuracy)/len(train_accuracy)}\")\n",
    "print(f\"\\nTrainLoss - {sum(train_losses)/len(train_losses)}\")\n",
    "print(f\"\\nValAccuracy - {sum(val_accuracy)/len(val_accuracy)}\")\n",
    "print(f\"\\nValLoss - {sum(val_losses)/len(val_losses)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-decision",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
