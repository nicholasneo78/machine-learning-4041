{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ordered-clearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation, Flatten, Dropout, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.utils import get_file\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import cvlib as cv\n",
    "import time\n",
    "\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "worthy-density",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device:/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# checking if GPU is being used for training\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "if tf.test.gpu_device_name(): \n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"GPU is not detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "horizontal-blues",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters \n",
    "epochs = 30\n",
    "lr = 1e-3\n",
    "batch_size = 32\n",
    "img_dims = (128,128,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "forced-colonial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from h5 file\n",
      "The keys are:  <KeysViewHDF5 ['image', 'label']>\n",
      "The shape of x_field (10222, 224, 224, 3)\n",
      "The shape of y_field (10222, 1)\n",
      "time taken 4s\n"
     ]
    }
   ],
   "source": [
    "# should be disable when testing on other models (image dimension maybe different)\n",
    "enable_load_saved = True;\n",
    "saved_file = \"precessed_data_{}x{}\".format(img_dims[0], img_dims[1])\n",
    "    \n",
    "data = []\n",
    "labels = []\n",
    "start = time.time()\n",
    "if (enable_load_saved and os.path.isfile(saved_file+\".h5\")):\n",
    "    print(\"loading from h5 file\")\n",
    "    data, labels = load_h5_data(saved_file+\".h5\")\n",
    "else :\n",
    "    print(\"please generate the h5 data file from main notebook\")\n",
    "end = time.time()\n",
    "print(\"time taken \"+time_convert(end-start))\n",
    "print(data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "digital-colors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset for training and validation\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.2,\n",
    "                                                  random_state=42)\n",
    "trainY = to_categorical(trainY, num_classes=120)\n",
    "testY = to_categorical(testY, num_classes=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "surrounded-sampling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmenting datset \n",
    "aug = ImageDataGenerator(rotation_range=25, width_shift_range=0.1,\n",
    "                         height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
    "                         horizontal_flip=True, fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "valued-evans",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "lr_rate = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-contrary",
   "metadata": {},
   "source": [
    "# Explore on common pre-trained models \n",
    "https://keras.io/api/applications/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "generic-packaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.applications import VGG16, VGG19\n",
    "from tensorflow.keras.applications import ResNet50, ResNet101, ResNet152, ResNet50V2, ResNet101V2, ResNet152V2\n",
    "from tensorflow.keras.applications import InceptionV3, InceptionResNetV2\n",
    "from tensorflow.keras.applications import MobileNet, MobileNetV2\n",
    "from tensorflow.keras.applications import DenseNet121, DenseNet169, DenseNet201\n",
    "from tensorflow.keras.applications import NASNetMobile, NASNetLarge\n",
    "\n",
    "\n",
    "# define dictionary of evaluating models\n",
    "models = {\n",
    "    # change here to evaluate other models\n",
    "    \"DenseNet121\": DenseNet121(include_top=False, weights='imagenet', input_shape=img_dims),\n",
    "    \"DenseNet169\": DenseNet169(include_top=False, weights='imagenet', input_shape=img_dims),\n",
    "    \"DenseNet201\": DenseNet201(include_top=False, weights='imagenet', input_shape=img_dims),\n",
    "    \"Xception\": Xception(include_top=False,weights=\"imagenet\", input_tensor=None, input_shape=img_dims, pooling=None, classes=1000, classifier_activation=\"softmax\"),\n",
    "    \"VGG16\": VGG16(include_top=False, weights='imagenet', input_shape=img_dims),\n",
    "    \"VGG19\": VGG19(include_top=False, weights='imagenet', input_shape=img_dims),\n",
    "    \"ResNet50\": ResNet50(include_top=False, weights='imagenet', input_shape=img_dims),\n",
    "    \"ResNet101\": ResNet101(include_top=False, weights='imagenet', input_shape=img_dims),\n",
    "    \"ResNet152\": ResNet152(include_top=False, weights='imagenet', input_shape=img_dims),\n",
    "    \"ResNet50V2\": ResNet50V2(include_top=False, weights='imagenet', input_shape=img_dims),\n",
    "    \"ResNet101V2\": ResNet101V2(include_top=False, weights='imagenet', input_shape=img_dims),\n",
    "    \"ResNet152V2\": ResNet152V2(include_top=False, weights='imagenet', input_shape=img_dims),\n",
    "    \"InceptionV3\": InceptionV3(include_top=False, weights='imagenet', input_shape=img_dims),\n",
    "    \"InceptionResNetV2\": InceptionResNetV2(include_top=False, weights='imagenet', input_shape=img_dims),\n",
    "    \"MobileNet\": MobileNet(include_top=False, weights='imagenet', input_shape=img_dims),\n",
    "    \"MobileNetV2\": MobileNetV2(include_top=False, weights='imagenet', input_shape=img_dims),\n",
    "    \n",
    "}\n",
    "\n",
    "# need to set the parametres\n",
    "#     \"NASNetMobile\": NASNetMobile(include_top=False, weights='imagenet', input_shape=img_dims),\n",
    "#     \"NASNetLarge\": NASNetLarge(include_top=False, weights='imagenet', input_shape=img_dims),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "micro-parks",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models:\n",
    "    # retrieve base model\n",
    "    base = models[model_name]\n",
    "    \n",
    "    # freeze pre-trained weight\n",
    "    base.trainable = False\n",
    "\n",
    "    # rebuild output layer\n",
    "    x = base.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    head = Dense(120, activation='softmax')(x)\n",
    "    model = Model(inputs=base.input, outputs=head)\n",
    "\n",
    "    # Compiling the model\n",
    "    model.compile(optimizer=Adam(lr=lr), \n",
    "                  loss = 'categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # start training models\n",
    "    start = time.time()\n",
    "    H=model.fit_generator(aug.flow(trainX, trainY, batch_size=batch_size),\n",
    "                            validation_data=(testX,testY),\n",
    "                            steps_per_epoch=len(trainX) // batch_size,\n",
    "                            epochs=epochs, verbose=1,\n",
    "                            callbacks=[lr_rate]\n",
    "                         )\n",
    "    end = time.time()\n",
    "    print(\"training time: \"+time_convert(end-start))\n",
    "    \n",
    "    # plot model performance\n",
    "    plot_model_history(H, saving_name=\"{}_freezed\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-burke",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
